{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {}
   },
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "Artificial Neural Networks (ANN) are inspired by the network structure of the human brain.\n",
    "They are built from individual *artificial neurons*, which are mathematical models of neurons \n",
    "that can be viewed as follows (cf. Russell & Norvig, Figure 18.19).\n",
    "\n",
    "<img src=\"an.png\" width=\"256px\"/>\n",
    "\n",
    "Notes:\n",
    "- The neuron receives *activation* from other input neurons ($a_i$).\n",
    "- A *bias* input can be added to each neuron ($a_0$).\n",
    "- Each input activation is weighted (i.e., the weight for input $i$ to neuron $j$ is $w_{ij}$).\n",
    "- The output activation is computed for each neuron ($a_j = g(in_j) = g(\\sum_{i=0}^n w_{ij} \\cdot a_i$)).\n",
    "\n",
    "Artificial neurons can be configured into *artificial neural networks*\n",
    "comprising multiple-layers. Here is an example.\n",
    "\n",
    "<img src=\"ann.png\" width=\"256px\"/>\n",
    "\n",
    "Notes:\n",
    "- This network architecture is:\n",
    "    - *Feed-forward*: The activation flows from the input (top) to the output (bottom).\n",
    "    - *Fully/Densely inter-connected*: The output of each node is fed into all nodes in the next layer.\n",
    "- Layers in such networks can be seen as non-linear functions of the outputs of the preceeding layers, i.e.: \n",
    "    $o = layer_h(layer_i(i))$\n",
    "\n",
    "We train these ANNs using the back-propagation algorithm:\n",
    "\n",
    "**1. Initialization**\n",
    "\n",
    "Set the ANN weights to \"small\", \"random\" numbers.\n",
    "\n",
    "**2. Feed-Forward**\n",
    "\n",
    "Take an example input from the training set and feed its activation through the network.\n",
    "\n",
    "$\n",
    "o_1 = \n",
    "\\left[\n",
    "\\begin{array}{c c}\n",
    "i_1 & i_2 \\\\ \n",
    "\\end{array}\n",
    "\\right]\n",
    "\\cdot\n",
    "\\left[\n",
    "\\begin{array}{c c}\n",
    "w_{i_1,h_1} & w_{i_1,h_2} \\\\ \n",
    "w_{i_2,h_1} & w_{i_2,h_2} \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\cdot\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "w_{h_1, o_1} \\\\ \n",
    "w_{h_2, o_1} \\\\ \n",
    "\\end{array}\n",
    "\\right]\n",
    "$\n",
    "\n",
    "**3. Error Computation**\n",
    "\n",
    "Compute the error using a common error function (e.g., L2 error) by comparing the desired output ($y_j$) with the actual output ($o_j$).\n",
    "\n",
    "$\n",
    "L_2Error = \\sum_{i=1}^n {(y_j - o_j)}^2 \n",
    "$\n",
    "\n",
    "**4. Back-Propagation** \n",
    "\n",
    "Modify the weights by propagating updates back through the network using a given learning rate ($rate$) and the raw errors generated by the output layer ($\\Delta_{o_j} = y_j - o_j$).\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "W_{i,h}^\\ast &\\leftarrow W_{i,h} + rate \\cdot A_{i} \\cdot g'(in_{h}) \\odot \\sum_k (W_{h, o} \\cdot \\Delta_{o}) \\\\\n",
    "W_{h,o}^\\ast &\\leftarrow W_{h,o} + rate \\cdot A_{h} \\cdot g'(in_{o}) \\cdot \\Delta_{o}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "These back-propagation formulae are derived by computing the following, the first for the hidden layer(s) (more complicated) and the second for the output layer (less complicated).\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "{{\\partial{Error_o}} \\over {\\partial{W_{i,h}}}} &= \\ldots = -A_i \\cdot g'(in_{h}) \\odot \\sum_k (W_{h, o} \\cdot \\Delta_{o_k}) \\\\\n",
    "{{\\partial{Error_o}} \\over {\\partial{W_{h,o}}}} &= \\ldots = -A_h \\cdot g'(in_{o}) \\cdot \\Delta_{o}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Interestingly, deep neural networks appear to work in practice without significant problems caused by local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Example\n",
    "\n",
    "In class, we ran the following example (inspired by [Backpropagation Step by Step](https://hmkcode.github.io/ai/backpropagation-step-by-step/)).\n",
    "\n",
    "1. Fill in random weights.\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    &\\begin{bmatrix}\n",
    "    w_{i_1,h_1} & w_{i_1,h_2} \\\\\n",
    "    w_{i_2,h_1} & w_{i_2,h_2}\n",
    "    \\end{bmatrix}\n",
    "    \\leftarrow\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} \\\\\n",
    "    &\\begin{bmatrix}\n",
    "    w_{h_1, o_1} \\\\ \n",
    "    w_{h_2, o_1} \n",
    "    \\end{bmatrix}\n",
    "    \\leftarrow\n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\\n",
    "    0.15\n",
    "    \\end{bmatrix}\n",
    "    \\end{aligned}$\n",
    "    \n",
    "2. Compute the output for one sample (XOR: `[0, 1]` &rarr; `1`).\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    o_j &= \n",
    "    \\begin{bmatrix}\n",
    "    0 & 1 \\\\ \n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\\n",
    "    0.15\n",
    "    \\end{bmatrix}\n",
    "    \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0 * 0.11 + 1 * 0.21 & 0 * 0.12 + 1 * 0.08\n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15\n",
    "    \\end{bmatrix}\n",
    "    \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15 \n",
    "    \\end{bmatrix}\n",
    "    \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.21 * 0.14 + 0.08 * 0.15\n",
    "    \\end{bmatrix}\n",
    "    \\\\ &= 0.0414 \n",
    "    \\end{aligned}\n",
    "    \\\\\n",
    "    $\n",
    "\n",
    "3. Compute the error (and, more importantly, the delta).\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    L_2Error &= (1 - 0.0414)^2 \\\\\n",
    "    &= 0.9189 \\\\\n",
    "    \\Delta_{o_1} &= (1 - 0.0414) \\\\\n",
    "    &= 0.9586 \\\\\n",
    "    \\end{aligned}$\n",
    "\n",
    "4. Back-propagate updates back through the network, assuming: \n",
    "    $learning\\_rate = 0.05$; \n",
    "    RELU activation functions for all nodes.\n",
    "     \n",
    "    $\\begin{aligned}\n",
    "    \\begin{bmatrix}\n",
    "    w_{h_1, o_1} \\\\ \n",
    "    w_{h_2, o_1}\n",
    "    \\end{bmatrix} &\\leftarrow \n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15 \n",
    "    \\end{bmatrix} + 0.05 \\cdot \n",
    "    \\begin{bmatrix}\n",
    "    0.21 \\\\ \n",
    "    0.08 \n",
    "    \\end{bmatrix} \\cdot 1.0 \\cdot 0.9586 \\\\\\\\\n",
    "    &= \n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15 \n",
    "    \\end{bmatrix} + \n",
    "    \\begin{bmatrix}\n",
    "    0.05 * 0.21 * 1.0 * 0.9586 \\\\\n",
    "    0.05 * 0.08 * 1.0 * 0.9586 \n",
    "    \\end{bmatrix} \\\\\n",
    "    &= \n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15 \n",
    "    \\end{bmatrix} +\n",
    "    \\begin{bmatrix}\n",
    "    0.0100 \\\\\n",
    "    0.00383 \n",
    "    \\end{bmatrix} \\\\\n",
    "    &=\n",
    "    \\begin{bmatrix}\n",
    "    0.1500 \\\\ \n",
    "    0.1538\n",
    "    \\end{bmatrix}\n",
    "    \\end{aligned}$\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    \\begin{bmatrix}\n",
    "    w_{i_1,h_1} & w_{i_1,h_2} \\\\ \n",
    "    w_{i_2,h_1} & w_{i_2,h_2}\n",
    "    \\end{bmatrix} &\\leftarrow \n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} + 0.05 \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0 & 0 \\\\ \n",
    "    1 & 1\n",
    "    \\end{bmatrix} \\cdot 1.0 \\odot\n",
    "    \\begin{bmatrix}\n",
    "    0.14 & 0.15 \\\\ \n",
    "    0.14 & 0.15\n",
    "    \\end{bmatrix} \\cdot 0.9586 \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} + \n",
    "    \\begin{bmatrix}\n",
    "    0.05 * 0 * 1.0 & 0.05 * 0 * 1.0 \\\\ \n",
    "    0.05 * 1 * 1.0 & 0.05 * 1 * 1.0 \\\\ \n",
    "    \\end{bmatrix} \\odot \n",
    "    \\begin{bmatrix}\n",
    "    0.14 * 0.9586 & 0.15 * 0.9586\\\\ \n",
    "    0.14 * 0.9586 & 0.15 * 0.9586 \n",
    "    \\end{bmatrix} \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} + \n",
    "    \\begin{bmatrix}\n",
    "    0.00 & 0.00 \\\\ \n",
    "    0.05 & 0.05 \n",
    "    \\end{bmatrix} \\odot \n",
    "    \\begin{bmatrix}\n",
    "    0.1342 & 0.1438 \\\\\n",
    "    0.1342 & 0.1438\n",
    "    \\end{bmatrix} \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} + \n",
    "    \\begin{bmatrix}\n",
    "    0.00 * 0.1342 & 0.00 * 0.1438 \\\\\n",
    "    0.05 * 0.1342 & 0.05 * 0.1438\n",
    "    \\end{bmatrix} \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} +     \n",
    "    \\begin{bmatrix}\n",
    "    0.0 & 0.0 \\\\\n",
    "    0.0067 & 0.0072\n",
    "    \\end{bmatrix} \\\\ &= \n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.2167 & 0.0872\n",
    "    \\end{bmatrix}  \n",
    "    \\end{aligned}$\n",
    "\n",
    "Notes:\n",
    "- We had to do 2 *broadcasts* (in the first line) to get the \n",
    "    required matrix dimensions. \n",
    "    - $A_i$: along the vertical (i.e., add duplicate column).\n",
    "    - $W_{h,o}$ along the horizontal (i.e., add duplicate row).\n",
    "    - This allows us to use element-wise multiplication, known as the \n",
    "        [Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) \n",
    "        and denoted by $\\odot$.\n",
    "- This process:\n",
    "    - works recursively for multiple layered networks.\n",
    "    - is more efficient than adjusting each weight individually.\n",
    "    - is generally run using *Mini-batch Stochastic Gradient Descent*.             "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
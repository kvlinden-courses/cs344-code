{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "Artificial Neural Networks (ANN) are inspired by the network structure of the human brain.\n",
    "They are built from individual artificial neurons, which are mathematical models of neurons \n",
    "that can be viewed as follows.\n",
    "\n",
    "<img src=\"https://cs.calvin.edu/courses/cs/344/kvlinden/07regression/images/an.png\" width=\"256px\"/>\n",
    "\n",
    "Notes:\n",
    "- The neuron receives *activation* from other input neurons ($a_i$).\n",
    "- A *bias* input can be added to each neuron ($a_0$).\n",
    "- Each input activation is weighted (i.e., the weight for input $i$ to neuron $j$ is $w_{ij}$).\n",
    "- The output activation is computed for each neuron ($a_j = g(in_j) = g(\\sum_{i=0}^n w_{ij} \\cdot a_i$)).\n",
    "\n",
    "Artificial neurons can be configured into feed-forward networks comprising multiple-layers. \n",
    "Here is an example.\n",
    "\n",
    "<img src=\"https://cs.calvin.edu/courses/cs/344/kvlinden/07regression/images/ann.png\" width=\"256px\"/>\n",
    "\n",
    "Notes:\n",
    "- This network architecture is:\n",
    "    - *Feed-forward*: The activation flows from the input (top) to the output (bottom).\n",
    "    - *Fully/Densely inter-connected*: The output of each node is fed into all nodes in the next layer.\n",
    "- Layers in such networks can be seen as non-linear functions of the outputs of the preceeding layers, i.e.: \n",
    "    $o = layer_h(layer_i(i))$\n",
    "\n",
    "We train these ANNs using the backpropagation algorithm:\n",
    "\n",
    "**1. Initialization**\n",
    "\n",
    "Set the ANN weights to \"small\", \"random\" numbers.\n",
    "\n",
    "**2. Feed-Forward**\n",
    "\n",
    "Take an example input from the training set and feed its activation through the network.\n",
    "\n",
    "$\n",
    "o_1 = \n",
    "\\left[\n",
    "\\begin{array}{c c}\n",
    "i_1 & i_2 \\\\ \n",
    "\\end{array}\n",
    "\\right]\n",
    "\\cdot\n",
    "\\left[\n",
    "\\begin{array}{c c}\n",
    "w_{i_1,h_1} & w_{i_1,h_2} \\\\ \n",
    "w_{i_2,h_1} & w_{i_2,h_2} \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\cdot\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "w_{h_1, o_1} \\\\ \n",
    "w_{h_2, o_1} \\\\ \n",
    "\\end{array}\n",
    "\\right]\n",
    "$\n",
    "\n",
    "**3. Error Computation**\n",
    "\n",
    "Compute the error using a common error function (e.g., L2 error) by comparing the desired output ($y_j$) with the actual output ($o_j$).\n",
    "\n",
    "$\n",
    "L2\\_Error = \\sum_{i=1}^n {(y_j - o_j)}^2 \n",
    "$\n",
    "\n",
    "**4. Backpropagation** \n",
    "\n",
    "Modify the weights by propagating updates back through the network using a given learning rate ($rate$) and the raw errors generated by the output layer ($\\Delta_{o_j} = y_j - o_j$).\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "W_{i,h}^\\ast &\\leftarrow W_{i,h} + rate \\cdot A_{i} \\cdot g'(in_{h}) \\odot \\sum_k (W_{h, o} \\cdot \\Delta_{o}) \\\\\n",
    "W_{h,o}^\\ast &\\leftarrow W_{h,o} + rate \\cdot A_{h} \\cdot g'(in_{o}) \\cdot \\Delta_{o}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "These backpropagation formulae are derived by computing the following, the first for the hidden layer(s) (more complicated) and the second for the output layer (less complicated).\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "{{\\partial{Error_o}} \\over {\\partial{W_{i,h}}}} &= \\ldots = -A_i \\cdot g'(in_{h}) \\odot \\sum_k (W_{h, o} \\cdot \\Delta_{o_k}) \\\\\n",
    "{{\\partial{Error_o}} \\over {\\partial{W_{h,o}}}} &= \\ldots = -A_h \\cdot g'(in_{o}) \\cdot \\Delta_{o}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Interestingly, deep neural networks appear to work in practice without significant problems caused by local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "In class, we ran the following example (inspired by [Backpropagation Step by Step](https://hmkcode.github.io/ai/backpropagation-step-by-step/)).\n",
    "\n",
    "1. Fill in random weights.\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    &\\begin{bmatrix}\n",
    "    w_{i_1,h_1} & w_{i_1,h_2} \\\\\n",
    "    w_{i_2,h_1} & w_{i_2,h_2}\n",
    "    \\end{bmatrix}\n",
    "    \\leftarrow\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} \\\\\n",
    "    &\\begin{bmatrix}\n",
    "    w_{h_1, o_1} \\\\ \n",
    "    w_{h_2, o_1} \n",
    "    \\end{bmatrix}\n",
    "    \\leftarrow\n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\\n",
    "    0.15\n",
    "    \\end{bmatrix}\n",
    "    \\end{aligned}$\n",
    "    \n",
    "2. Compute the output for one sample (XOR: `[0, 1]` &rarr; `1`).\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    o_j &= \n",
    "    \\begin{bmatrix}\n",
    "    0 & 1 \\\\ \n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\\n",
    "    0.15\n",
    "    \\end{bmatrix}\n",
    "    \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0 * 0.11 + 1 * 0.21 & 0 * 0.12 + 1 * 0.08\n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15\n",
    "    \\end{bmatrix}\n",
    "    \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15 \n",
    "    \\end{bmatrix}\n",
    "    \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.21 * 0.14 + 0.08 * 0.15\n",
    "    \\end{bmatrix}\n",
    "    \\\\ &= 0.0414 \n",
    "    \\end{aligned}\n",
    "    \\\\\n",
    "    $\n",
    "\n",
    "3. Compute the error (and more importantly, the delta).\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    L2Error &= {(1 - 0.0414)}^2 \\\\\n",
    "            &= 0.9189 \\\\ \n",
    "    \\Delta_{o_1} &= (1 - 0.0414) \\\\\n",
    "                 &= 0.9586\n",
    "    \\end{aligned}$\n",
    "\n",
    "4. Backpropagate updates back through the network, assuming: \n",
    "    $learning\\_rate = 0.05$; \n",
    "    RELU activation functions for all nodes.\n",
    "     \n",
    "    $\\begin{aligned}\n",
    "    \\begin{bmatrix}\n",
    "    w_{h_1, o_1} \\\\ \n",
    "    w_{h_2, o_1}\n",
    "    \\end{bmatrix} &\\leftarrow \n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15 \n",
    "    \\end{bmatrix} + 0.05 \\cdot \n",
    "    \\begin{bmatrix}\n",
    "    0.21 \\\\ \n",
    "    0.08 \n",
    "    \\end{bmatrix} \\cdot 1.0 \\cdot 0.9586 \\\\\\\\\n",
    "    &= \n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15 \n",
    "    \\end{bmatrix} + \n",
    "    \\begin{bmatrix}\n",
    "    0.05 * 0.21 * 1.0 * 0.9586 \\\\\n",
    "    0.05 * 0.08 * 1.0 * 0.9586 \n",
    "    \\end{bmatrix} \\\\\n",
    "    &= \n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15 \n",
    "    \\end{bmatrix} +\n",
    "    \\begin{bmatrix}\n",
    "    0.0100 \\\\\n",
    "    0.00383 \n",
    "    \\end{bmatrix} \\\\\n",
    "    &=\n",
    "    \\begin{bmatrix}\n",
    "    0.1500 \\\\ \n",
    "    0.1538\n",
    "    \\end{bmatrix}\n",
    "    \\end{aligned}$\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    \\begin{bmatrix}\n",
    "    w_{i_1,h_1} & w_{i_1,h_2} \\\\ \n",
    "    w_{i_2,h_1} & w_{i_2,h_2}\n",
    "    \\end{bmatrix} &\\leftarrow \n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} + 0.05 \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0 & 0 \\\\ \n",
    "    1 & 1\n",
    "    \\end{bmatrix} \\cdot 1.0 \\odot\n",
    "    \\begin{bmatrix}\n",
    "    0.14 & 0.15 \\\\ \n",
    "    0.14 & 0.15\n",
    "    \\end{bmatrix} \\cdot 0.9586 \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} + \n",
    "    \\begin{bmatrix}\n",
    "    0.05 * 0 * 1.0 & 0.05 * 0 * 1.0 \\\\ \n",
    "    0.05 * 1 * 1.0 & 0.05 * 1 * 1.0 \\\\ \n",
    "    \\end{bmatrix} \\odot \n",
    "    \\begin{bmatrix}\n",
    "    0.14 * 0.9586 & 0.15 * 0.9586\\\\ \n",
    "    0.14 * 0.9586 & 0.15 * 0.9586 \n",
    "    \\end{bmatrix} \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} + \n",
    "    \\begin{bmatrix}\n",
    "    0.00 & 0.00 \\\\ \n",
    "    0.05 & 0.05 \n",
    "    \\end{bmatrix} \\odot \n",
    "    \\begin{bmatrix}\n",
    "    0.1342 & 0.1438 \\\\\n",
    "    0.1342 & 0.1438\n",
    "    \\end{bmatrix} \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} + \n",
    "    \\begin{bmatrix}\n",
    "    0.00 * 0.1342 & 0.00 * 0.1438 \\\\\n",
    "    0.05 * 0.1342 & 0.05 * 0.1438\n",
    "    \\end{bmatrix} \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} +     \n",
    "    \\begin{bmatrix}\n",
    "    0.0 & 0.0 \\\\\n",
    "    0.0067 & 0.0072\n",
    "    \\end{bmatrix} \\\\ &= \n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.2167 & 0.0872\n",
    "    \\end{bmatrix}  \n",
    "    \\end{aligned}$\n",
    "\n",
    "Notes:\n",
    "- We had to do 2 *broadcasts* (in the first line) to get the \n",
    "    required matrix dimensions. \n",
    "    - $A_i$: along the vertical (i.e., add duplicate column).\n",
    "    - $W_{h,o}$ along the horizontal (i.e., add duplicate row).\n",
    "- This process:\n",
    "    - works recursively for multiple layered networks.\n",
    "    - is more efficient that adjusting each weight individually.\n",
    "    - is generally run using *Mini-batch Stochastic Gradient Descent*.             "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
